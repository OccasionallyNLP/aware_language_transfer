{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7869d9-5a7a-4a5c-89e4-41fcee597e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dec6cce-024d-46c5-8c55-757986a8b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3000it [00:37, 80.67it/s]                                                                                         | 0/1000 [00:00<?, ?it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d500e4c4-8326-40e1-a3d7-fd1e4b69fc6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'dict' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1000\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mprogress_bar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstep\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\tqdm\\std.py:1227\u001b[39m, in \u001b[36mtqdm.update\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.disable:\n\u001b[32m   1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m:\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28mself\u001b[39m.last_print_n += n  \u001b[38;5;66;03m# for auto-refresh logic to work\u001b[39;00m\n\u001b[32m   1229\u001b[39m \u001b[38;5;28mself\u001b[39m.n += n\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'dict' and 'int'"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    progress_bar.update({'step':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b220f10a-919f-457a-8eb6-a0896b47cdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d98a82-1f77-4147-b42b-8dfce2f530da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from datasets import (\n",
    "        Dataset,\n",
    "        DatasetDict,\n",
    "        Features,\n",
    "        Sequence,\n",
    "        Value,\n",
    "        concatenate_datasets,\n",
    "        load_dataset,\n",
    "    )\n",
    "except ImportError:\n",
    "    warnings.warn(\"Datasets not installed, you'll be unable to use these dataset processing functions.\")\n",
    "\n",
    "# Import SFT processing functions for backward compatibility\n",
    "\n",
    "\n",
    "def clm_process(\n",
    "    raw_dataset: \"Dataset\",\n",
    "    tokenizer,\n",
    "    text_column_name: str,\n",
    "    dataset_processing_num_proc_per_process: int,\n",
    "    dataset_overwrite_cache: bool,\n",
    "    sequence_length: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenate all texts from raw_dataset and generate chunks of `sequence_length + 1`,\n",
    "    where chunks overlap by a single token.\n",
    "\n",
    "    Args:\n",
    "        raw_dataset: Dataset containing raw text\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        text_column_name: Name of the column containing text data\n",
    "        dataset_processing_num_proc_per_process: Number of processes for parallelization\n",
    "        dataset_overwrite_cache: Whether to overwrite the cache\n",
    "        sequence_length: Maximum sequence length\n",
    "\n",
    "    Returns:\n",
    "        Processed dataset with tokenized sequences\n",
    "    \"\"\"\n",
    "    # Adapted from https://github.com/huggingface/transformers/blob/47e1676255e5dd86b9541f734cd4f4bdcbb50f4a/examples/pytorch/language-modeling/run_clm.py#L391-L439\n",
    "\n",
    "    def group_texts(examples: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: np.concatenate(v) for k, v in examples.items()}\n",
    "        total_length = len(concatenated_examples[next(iter(examples.keys()))])\n",
    "        # WARNING: We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= sequence_length + 1:\n",
    "            total_length = ((total_length - 1) // sequence_length) * sequence_length + 1\n",
    "        # Split by chunks of sequence_length.\n",
    "        result = {\n",
    "            k: [\n",
    "                t[i : i + sequence_length + 1] for i in range(0, total_length - (sequence_length + 1), sequence_length)\n",
    "            ]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    def _tokenize_and_group_texts(texts: List[str]) -> Dict[str, List[np.ndarray]]:\n",
    "        # add_eos_token\n",
    "        texts = [i+tokenizer.eos_token for i in texts]\n",
    "        # print(texts[0])\n",
    "        tokenized_batch = tokenizer.batch_encode_plus(texts, add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False) # add_special_tokens = False\n",
    "        tokenized_batch = {k: [np.array(tokenized_texts) for tokenized_texts in v] for k, v in tokenized_batch.items()}\n",
    "        return group_texts(tokenized_batch)\n",
    "\n",
    "    train_dataset = raw_dataset.map(\n",
    "        _tokenize_and_group_texts,\n",
    "        input_columns=text_column_name,\n",
    "        remove_columns=raw_dataset.column_names,\n",
    "        features=Features({\"input_ids\": Sequence(feature=Value(dtype=\"int64\"), length=sequence_length + 1)}),\n",
    "        batched=True,\n",
    "        num_proc=dataset_processing_num_proc_per_process,\n",
    "        load_from_cache_file=not dataset_overwrite_cache,\n",
    "        desc=f\"Grouping texts in chunks of {sequence_length+1}\",\n",
    "    )\n",
    "    return train_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d757d0-5060-4d50-8724-6ea71a8292c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45b768b3c084544a45dc6c77ced109c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "text_column_name = 'text'\n",
    "\n",
    "num_proc = 1\n",
    "\n",
    "from datasets import load_dataset\n",
    "# get Croatian data\n",
    "ds = load_dataset(\"HuggingFaceFW/fineweb-2\", name=\"kor_Hang\", streaming=True, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9628d0b-2570-48c4-a382-79e60b755d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24ebc46-8ce2-4ecd-9768-b02b871365b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ds, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ee0e63-1a87-45ba-a16d-812c37cd92b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:07, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:33\u001b[39m, in \u001b[36m_IterableDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         data.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28mself\u001b[39m.ended = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\datasets\\iterable_dataset.py:2266\u001b[39m, in \u001b[36mIterableDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2263\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m formatter.format_row(pa_table)\n\u001b[32m   2264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2266\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2267\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no need to format thanks to FormattedExamplesIterable\u001b[39;49;00m\n\u001b[32m   2268\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\datasets\\iterable_dataset.py:1856\u001b[39m, in \u001b[36mFormattedExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1849\u001b[39m     formatter = get_formatter(\n\u001b[32m   1850\u001b[39m         \u001b[38;5;28mself\u001b[39m.formatting.format_type,\n\u001b[32m   1851\u001b[39m         features=\u001b[38;5;28mself\u001b[39m._features \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable.is_typed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1852\u001b[39m         token_per_repo_id=\u001b[38;5;28mself\u001b[39m.token_per_repo_id,\n\u001b[32m   1853\u001b[39m     )\n\u001b[32m   1854\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable.iter_arrow:\n\u001b[32m   1855\u001b[39m     \u001b[38;5;66;03m# feature casting (inc column addition) handled within self._iter_arrow()\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1856\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_batch_to_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\datasets\\iterable_dataset.py:1879\u001b[39m, in \u001b[36mFormattedExamplesIterable._iter_arrow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1877\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.features:\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable._iter_arrow()\n\u001b[32m-> \u001b[39m\u001b[32m1879\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_iter_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43marrow_schema\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\datasets\\iterable_dataset.py:323\u001b[39m, in \u001b[36mArrowExamplesIterable._iter_arrow\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    321\u001b[39m shard_example_idx_start = \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mshard_example_idx\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    322\u001b[39m shard_example_idx = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_tables_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshard_example_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshard_example_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_example_idx_start\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\datasets\\packaged_modules\\parquet\\parquet.py:93\u001b[39m, in \u001b[36mParquet._generate_tables\u001b[39m\u001b[34m(self, files)\u001b[39m\n\u001b[32m     91\u001b[39m batch_size = \u001b[38;5;28mself\u001b[39m.config.batch_size \u001b[38;5;129;01mor\u001b[39;00m parquet_fragment.row_groups[\u001b[32m0\u001b[39m].num_rows\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparquet_fragment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mfilter_expr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_readahead\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfragment_readahead\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrecord_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;49;00m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;49;00m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3830\u001b[39m, in \u001b[36m_iterator\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3439\u001b[39m, in \u001b[36mpyarrow._dataset.TaggedRecordBatchIterator.__next__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(_cls, record_batch, fragment)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(dataloader):\n",
    "    i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c7b83b6-8049-44a8-b70c-968af9ac8dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/tiny_llama\\\\tokenizer_config.json',\n",
       " './data/tiny_llama\\\\special_tokens_map.json',\n",
       " './data/tiny_llama\\\\tokenizer.model',\n",
       " './data/tiny_llama\\\\added_tokens.json',\n",
       " './data/tiny_llama\\\\tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./data/tiny_llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "996a26be-2cc2-4088-89b5-b38442a3fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in ds:\n",
    "    if len(data)==1000:\n",
    "        break\n",
    "    data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c4e3658-d148-4310-8562-0fa8970b7cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6628aba1-ced5-4aea-9992-ca5c5f3bdde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07ffd6b4-3ffc-4b7f-9af5-d7718f615afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef12eb5ffb94b7e95e6453269fe2565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1025:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For pretraining, use existing CLM processing\n",
    "train_dataset = clm_process(\n",
    "    raw_dataset = ds,\n",
    "    tokenizer = tokenizer,\n",
    "    text_column_name = 'text',\n",
    "    dataset_processing_num_proc_per_process = num_proc,\n",
    "    dataset_overwrite_cache = True,\n",
    "    sequence_length = 1024,\n",
    ")\n",
    "\n",
    "# # For pretraining, use existing CLM processing\n",
    "# test_dataset = clm_process(\n",
    "#     raw_dataset = ds['test'],\n",
    "#     tokenizer = tokenizer,\n",
    "#     text_column_name = 'text',\n",
    "#     dataset_processing_num_proc_per_process = num_proc,\n",
    "#     dataset_overwrite_cache = True,\n",
    "#     sequence_length = 1024('./data/tokenized/fineweb-2-ko/train')('./data/tokenized/fineweb-2-ko/train'),\n",
    "# )\n",
    "\n",
    "# # train-test split for english \n",
    "# en_train, en_test = ds_en['train'].train_test_split(test_size=0.001)\n",
    "\n",
    "\n",
    "\n",
    "# # For pretraining, use existing CLM processing\n",
    "# en_train_dataset = clm_process(\n",
    "#     raw_dataset = ds_en['train'],\n",
    "#     tokenizer = tokenizer,\n",
    "#     text_column_name = 'text',\n",
    "#     dataset_processing_num_proc_per_process = num_proc,\n",
    "#     dataset_overwrite_cache = True,\n",
    "#     sequence_length = 1024,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a974ce4c-2cba-4a9d-bc15-5f3b6abbc2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17845838efcd42c7a95b1cd35b3f7804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.save_to_disk('./data/tokenized/fineweb-2-ko/test') #('./data/tokenized/fineweb-2-ko/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb90cf5e-421a-4d1d-ba1a-1822bb8371d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f7455b-efec-488b-a67d-66f15b0bb5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_disk("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc254ed-fd8f-4875-9739-05796e9cf6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save_to_disk('"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
