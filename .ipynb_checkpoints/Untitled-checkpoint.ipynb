{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7869d9-5a7a-4a5c-89e4-41fcee597e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dec6cce-024d-46c5-8c55-757986a8b225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3000it [00:37, 80.67it/s]                                                                                         | 0/1000 [00:00<?, ?it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d500e4c4-8326-40e1-a3d7-fd1e4b69fc6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'dict' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1000\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mprogress_bar\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstep\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Ok\\lang_t\\Lib\\site-packages\\tqdm\\std.py:1227\u001b[39m, in \u001b[36mtqdm.update\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.disable:\n\u001b[32m   1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m:\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28mself\u001b[39m.last_print_n += n  \u001b[38;5;66;03m# for auto-refresh logic to work\u001b[39;00m\n\u001b[32m   1229\u001b[39m \u001b[38;5;28mself\u001b[39m.n += n\n",
      "\u001b[31mTypeError\u001b[39m: '<' not supported between instances of 'dict' and 'int'"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    progress_bar.update({'step':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b220f10a-919f-457a-8eb6-a0896b47cdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d98a82-1f77-4147-b42b-8dfce2f530da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from datasets import (\n",
    "        Dataset,\n",
    "        DatasetDict,\n",
    "        Features,\n",
    "        Sequence,\n",
    "        Value,\n",
    "        concatenate_datasets,\n",
    "        load_dataset,\n",
    "    )\n",
    "except ImportError:\n",
    "    warnings.warn(\"Datasets not installed, you'll be unable to use these dataset processing functions.\")\n",
    "\n",
    "# Import SFT processing functions for backward compatibility\n",
    "\n",
    "\n",
    "def clm_process(\n",
    "    raw_dataset: \"Dataset\",\n",
    "    tokenizer,\n",
    "    text_column_name: str,\n",
    "    dataset_processing_num_proc_per_process: int,\n",
    "    dataset_overwrite_cache: bool,\n",
    "    sequence_length: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenate all texts from raw_dataset and generate chunks of `sequence_length + 1`,\n",
    "    where chunks overlap by a single token.\n",
    "\n",
    "    Args:\n",
    "        raw_dataset: Dataset containing raw text\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        text_column_name: Name of the column containing text data\n",
    "        dataset_processing_num_proc_per_process: Number of processes for parallelization\n",
    "        dataset_overwrite_cache: Whether to overwrite the cache\n",
    "        sequence_length: Maximum sequence length\n",
    "\n",
    "    Returns:\n",
    "        Processed dataset with tokenized sequences\n",
    "    \"\"\"\n",
    "    # Adapted from https://github.com/huggingface/transformers/blob/47e1676255e5dd86b9541f734cd4f4bdcbb50f4a/examples/pytorch/language-modeling/run_clm.py#L391-L439\n",
    "\n",
    "    def group_texts(examples: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: np.concatenate(v) for k, v in examples.items()}\n",
    "        total_length = len(concatenated_examples[next(iter(examples.keys()))])\n",
    "        # WARNING: We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= sequence_length + 1:\n",
    "            total_length = ((total_length - 1) // sequence_length) * sequence_length + 1\n",
    "        # Split by chunks of sequence_length.\n",
    "        result = {\n",
    "            k: [\n",
    "                t[i : i + sequence_length + 1] for i in range(0, total_length - (sequence_length + 1), sequence_length)\n",
    "            ]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    def _tokenize_and_group_texts(texts: List[str]) -> Dict[str, List[np.ndarray]]:\n",
    "        # add_eos_token\n",
    "        texts = [i+tokenizer.eos_token for i in texts]\n",
    "        # print(texts[0])\n",
    "        tokenized_batch = tokenizer.batch_encode_plus(texts, add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False) # add_special_tokens = False\n",
    "        tokenized_batch = {k: [np.array(tokenized_texts) for tokenized_texts in v] for k, v in tokenized_batch.items()}\n",
    "        return group_texts(tokenized_batch)\n",
    "\n",
    "    train_dataset = raw_dataset.map(\n",
    "        _tokenize_and_group_texts,\n",
    "        input_columns=text_column_name,\n",
    "        remove_columns=raw_dataset.column_names,\n",
    "        features=Features({\"input_ids\": Sequence(feature=Value(dtype=\"int64\"), length=sequence_length + 1)}),\n",
    "        batched=True,\n",
    "        num_proc=dataset_processing_num_proc_per_process,\n",
    "        load_from_cache_file=not dataset_overwrite_cache,\n",
    "        desc=f\"Grouping texts in chunks of {sequence_length+1}\",\n",
    "    )\n",
    "    return train_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d757d0-5060-4d50-8724-6ea71a8292c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3b6f9a3fcf48658108aa0a17084e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Ok\\lang_t\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--google--gemma-2b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb5206c98d941f6b31b6a7146c661f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9444c46176c6408e8b154a1ed8aa3721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319e6c32a37f445f956a48f35df8ff26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fda21f974c047008e6b50dbcfd16788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "text_column_name = 'text'\n",
    "\n",
    "num_proc = 1\n",
    "\n",
    "from datasets import load_dataset\n",
    "# get Croatian data\n",
    "ds = load_dataset(\"HuggingFaceFW/fineweb-2\", name=\"kor_Hang\", streaming=True, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c7b83b6-8049-44a8-b70c-968af9ac8dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/tiny_llama\\\\tokenizer_config.json',\n",
       " './data/tiny_llama\\\\special_tokens_map.json',\n",
       " './data/tiny_llama\\\\tokenizer.model',\n",
       " './data/tiny_llama\\\\added_tokens.json',\n",
       " './data/tiny_llama\\\\tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./data/tiny_llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "996a26be-2cc2-4088-89b5-b38442a3fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in ds:\n",
    "    if len(data)==1000:\n",
    "        break\n",
    "    data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c4e3658-d148-4310-8562-0fa8970b7cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6628aba1-ced5-4aea-9992-ca5c5f3bdde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07ffd6b4-3ffc-4b7f-9af5-d7718f615afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef12eb5ffb94b7e95e6453269fe2565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 1025:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For pretraining, use existing CLM processing\n",
    "train_dataset = clm_process(\n",
    "    raw_dataset = ds,\n",
    "    tokenizer = tokenizer,\n",
    "    text_column_name = 'text',\n",
    "    dataset_processing_num_proc_per_process = num_proc,\n",
    "    dataset_overwrite_cache = True,\n",
    "    sequence_length = 1024,\n",
    ")\n",
    "\n",
    "# # For pretraining, use existing CLM processing\n",
    "# test_dataset = clm_process(\n",
    "#     raw_dataset = ds['test'],\n",
    "#     tokenizer = tokenizer,\n",
    "#     text_column_name = 'text',\n",
    "#     dataset_processing_num_proc_per_process = num_proc,\n",
    "#     dataset_overwrite_cache = True,\n",
    "#     sequence_length = 1024('./data/tokenized/fineweb-2-ko/train')('./data/tokenized/fineweb-2-ko/train'),\n",
    "# )\n",
    "\n",
    "# # train-test split for english \n",
    "# en_train, en_test = ds_en['train'].train_test_split(test_size=0.001)\n",
    "\n",
    "\n",
    "\n",
    "# # For pretraining, use existing CLM processing\n",
    "# en_train_dataset = clm_process(\n",
    "#     raw_dataset = ds_en['train'],\n",
    "#     tokenizer = tokenizer,\n",
    "#     text_column_name = 'text',\n",
    "#     dataset_processing_num_proc_per_process = num_proc,\n",
    "#     dataset_overwrite_cache = True,\n",
    "#     sequence_length = 1024,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a974ce4c-2cba-4a9d-bc15-5f3b6abbc2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17845838efcd42c7a95b1cd35b3f7804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1151 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.save_to_disk('./data/tokenized/fineweb-2-ko/test') #('./data/tokenized/fineweb-2-ko/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb90cf5e-421a-4d1d-ba1a-1822bb8371d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f7455b-efec-488b-a67d-66f15b0bb5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_disk("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc254ed-fd8f-4875-9739-05796e9cf6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.save_to_disk('"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
