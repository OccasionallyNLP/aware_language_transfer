accelerate launch train.py --config_name micro_llama --tokenizer_name micro_llama --dataset_config_path configs/token_based.yml --output_dir output --max_train_steps 3000 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --learning_rate 3e-4 --weight_decay 0.01 --num_warmup_steps 100 --gradient_accumulation_steps 1 --evaluation_steps 5 --block_size 1024 
